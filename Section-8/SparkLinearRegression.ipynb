{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create a SparkSession (Note, the config section is only for Windows!)\n",
    "    spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"LinearRegression\").getOrCreate()\n",
    "\n",
    "    # Load up our data and convert it to the format MLLib expects.\n",
    "    inputLines = spark.sparkContext.textFile(\"regression.txt\")\n",
    "    data = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))\n",
    "\n",
    "    # Convert this RDD to a DataFrame\n",
    "    colNames = [\"label\", \"features\"]\n",
    "    df = data.toDF(colNames)\n",
    "\n",
    "    # Note, there are lots of cases where you can avoid going from an RDD to a DataFrame.\n",
    "    # Perhaps you're importing data from a real database. Or you are using structured streaming\n",
    "    # to get your data.\n",
    "\n",
    "    # Let's split our data into training data and testing data\n",
    "    trainTest = df.randomSplit([0.5, 0.5])\n",
    "    trainingDF = trainTest[0]\n",
    "    testDF = trainTest[1]\n",
    "\n",
    "    # Now create our linear regression model\n",
    "    lir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "    # Train the model using our training data\n",
    "    model = lir.fit(trainingDF)\n",
    "\n",
    "    # Now see if we can predict values in our test data.\n",
    "    # Generate predictions using our linear regression model for all features in our\n",
    "    # test dataframe:\n",
    "    fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "    # Extract the predictions and the \"known\" correct labels.\n",
    "    predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "    labels = fullPredictions.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "    # Zip them together\n",
    "    predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "    # Print out the predicted and actual values for each point\n",
    "    for prediction in predictionAndLabel:\n",
    "      print(prediction)\n",
    "\n",
    "\n",
    "    # Stop the session\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
